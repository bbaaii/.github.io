<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Yu Deng">
  <meta name="description" content="Yu Deng's Homepage">
  <meta name="keywords" content="Yu Deng,邓誉,homepage,主页,PhD,computer vision,MSRA,Tsinghua,3D face,3D reconstruction,image generation,implicit field">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yunpeng Bai (白云鹏)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yunpeng Bai (白云鹏)</name>
              </p>
              <p style="text-align:center">
                Email: byp20[at]mails.tsinghua.edu.cn &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=OJNkfm8AAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/bbaaii">Github</a>
              </p>
              <p>I am a second-year M.E. student in Computer Technology at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. My research interests include neural rendering, image restoration&generation, and neural representation. I am currently a research intern at Tencent AI Lab. I also work closely with Prof. <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a> and <a href="https://xuanwangvc.github.io/">Xuan Wang</a>. Before joining Tsinghua University, I received B.Sc. in Computer Science from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>. 

              </p>
              <p>
                <strong>
                  I am actively looking for a Ph.D. position in computer graphics/vision for 2023 fall.
                </strong>
              </p>
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/yunpeng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yunpeng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/PS-NeRV.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>PS-NeRV: Patch-wise Stylized Neural Representations for Videos</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, Chao Dong, Cairong Wang, Chen Tang
                <br>
                <em>arXiv</em> 2022,
				<!-- <strong><font color="#FF0000">Oral Presentation</font></strong> -->
                <br>
                <a href="https://arxiv.org/abs/2208.03742">[PDF]</a>
                <!-- <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a> -->
				<!-- <a href="images/discoface.txt">[BibTeX]</a> -->
                <br>
                <p>We study how to represent a video with implicit neural representations (INRs). Classical INRs methods generally utilize MLPs to map input coordinates to output pixels. While some recent works have tried to directly reconstruct the whole image with CNNs. However, we argue that both the above pixel-wise and image-wise strategies are not favorable to video data. Instead, we propose a patch-wise solution, PS-NeRV, which represents videos as a function of patches and the corresponding patch coordinate. It naturally inherits the advantages of image-wise methods, and achieves excellent reconstruction performance with fast decoding speed. </p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/style.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Improving the Latent Space of Image Style Transfer</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, Cairong Wang, Chun Yuan
                <br>
                <em>arXiv</em> 2022,
                <br>
                <a href="https://arxiv.org/pdf/2205.12135.pdf">[PDF]</a>
                <!-- <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a> -->
				<!-- <a href="images/deep3dportrait.txt">[BibTeX]</a> -->
                <br>
                <p>Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task.</p>
            </td>
        </tr>
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                  <img src='images/eccv2022.jpg' style="width:100%;max-width:100%; position: absolute;top: -5%">
		<!-- <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/accurate3d.mp4'>
		</video> -->
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Semantic-Sparse Colorization Network for Deep Exemplar-based Colorization</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, Chao Dong, Zenghao Chai, Andong Wang, Zhengzhuo Xu, Chun Yuan
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2022, 
                <!-- <strong><font color="#FF0000">Best Paper Award</font></strong> -->
                <br>
                <a href="https://arxiv.org/pdf/2112.01335.pdf">[PDF]</a>
                <a href="https://github.com/bbaaii/SSC-Net">[Code]</a>
				<!-- <a href="images/accurate3d.txt">[BibTeX]</a> -->
                <br>
                <p>We propose Semantic-Sparse Colorization Network (SSCN) to transfer both the global image style and detailed semantic-related colors to the gray-scale image in a coarse-to-fine manner. Our network can perfectly balance the global and local colors while alleviating the ambiguous matching problem.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src='images/cms-lstm.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning</papertitle>
                <br>
                Zenghao Chai, Zhengzhuo Xu, <strong>Yunpeng Bai</strong>, Zhihui Lin, Chun Yuan
                <br>
                <em>IEEE International Conference on Multimedia and Expo</em>, ICME 2022,
                <br>
                <a href="https://arxiv.org/pdf/2102.03586.pdf">[PDF]</a>
                <a href="https://github.com/czh-98/CMS-LSTM">[Code]</a>
				<!-- <a href="images/dif_net.txt">[BibTeX]</a> -->
                <br>
                <p>To tackle the increasing ambiguity during forecasting, we design CMS-LSTM to focus on context correlations and multi-scale spatiotemporal flow with details on fine-grained locals, containing two elaborate designed blocks: Context Embedding (CE) and Spatiotemporal Expression (SE) blocks. CE is designed for abundant context interactions, while SE focuses on multi-scale spatiotemporal expression in hidden states.</p>
            </td>
        </tr>
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src="images/TRNet.png" style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Latent Tangent Space Representation for Normal Estimation</papertitle>
              <br>
              Junjie Cao, Hairui Zhu, <strong>Yunpeng Bai</strong>, Jun Zhou, Jinshan Pan, Zhixun Su   
              <br>
              <em>IEEE Transactions on Industrial Electronics (TIE)</em>, 2022, 69(1), 921-929, 
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9339852">[PDF]</a>
              <a href="https://github.com/jjcao/TRNET19">[Code]</a>
              <br>
              <p>We propose a simple deep network to estimate the normal vector based on a latent tangent space representation learned in the network. We call the network tangent represent learning network (TRNet). For each query point, the tangent space representation is a set of latent points spanning the tangent plane of it. The representation is generated using only the coordinates of its neighbors and regularized by a differentiable random sample consensus like component, which makes TRNet more compact and effective for normal estimation. </p>
            </td>
          </tr>
        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
